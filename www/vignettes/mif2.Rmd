---
title: "An IF2 example"
author: "Aaron A. King"
output:
  html_document:
    theme: flatly
    toc: yes
bibliography: pomp.bib
csl: ecology.csl
---

Licensed under the [Creative Commons attribution-noncommercial license](http://creativecommons.org/licenses/by-nc/3.0).
Please share and remix noncommercially, mentioning its origin.  
![CC-BY_NC](../graphics/cc-by-nc.png)

This document was produced using `pomp` version `r packageVersion("pomp")`.

```{r options,include=FALSE,cache=FALSE,purl=FALSE}
library(knitr)
prefix <- "mif2"
opts_chunk$set(
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=TRUE,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",prefix,"-"),
  cache.path=paste0("cache/",prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
  )
```
```{r prelims,echo=FALSE,cache=FALSE}
require(ggplot2)
require(plyr)
require(reshape2)
require(magrittr)
theme_set(theme_bw())
require(pomp)
stopifnot(packageVersion("pomp")>="0.65-1")
options(
  keep.source=TRUE,
  stringsAsFactors=FALSE,
  encoding="UTF-8",
  scipen=5,
  cores=5
  )
```

Iterated filtering is a technique for maximizing the likelihood obtained by filtering.
In `pomp`, it is the particle filter that is iterated.
Iterated filtering is implemented in the `mif` function.
@Ionides2015 describe an improvement on the original [@Ionides2006] algorithm.
This "IF2" algorithm is currently implemented by setting the `method="mif2"` option in `mif`.

Let's use IF2 to obtain an approximate MLE for the data in the Gompertz model example provided with `pomp`.
We'll initialize the algorithm at several starting points around `theta.true` and just estimate the parameters $r$, $\tau$, and $\sigma$:
```{r gompertz-init,cache=FALSE}
require(pomp)
pompExample(gompertz)
theta <- coef(gompertz)
theta.true <- theta
```

Note that we've set `transform=TRUE` in the above.
This means that the likelihood maximization is done on the estimation scale (see the section above on Parameter Transformations).

```{r gompertz-multi-mif2-eval,results='hide'}
require(foreach)
require(doMC)
registerDoMC()

save.seed <- .Random.seed
set.seed(334388458L,kind="L'Ecuyer")

estpars <- c("r","sigma","tau")
mf <- foreach(i=1:10,
              .inorder=FALSE,
              .options.multicore=list(set.seed=TRUE)
              ) %dopar%
  {
    theta.guess <- theta.true
    theta.guess[estpars] <- rlnorm(
      n=length(estpars),
      meanlog=log(theta.guess[estpars]),
      sdlog=1
      )
    m1 <- mif(
      gompertz,
      Nmif=50,
      method="mif2",
      start=theta.guess,
      transform=TRUE,
      rw.sd=c(r=0.02,sigma=0.02,tau=0.05),
      Np=2000,
      var.factor=2,
      cooling.type="hyperbolic",
      cooling.fraction=0.95
      )
    m1 <- continue(m1,Nmif=50,cooling.fraction=0.8)
    m1 <- continue(m1,Nmif=50,cooling.fraction=0.6)
    m1 <- continue(m1,Nmif=50,cooling.fraction=0.2)
    ll <- replicate(n=10,logLik(pfilter(m1,Np=10000)))
    list(mif=m1,ll=ll)
    }
```
```{r gompertz-post-mif2}
loglik.true <- replicate(n=10,logLik(pfilter(gompertz,Np=10000)))
loglik.true <- logmeanexp(loglik.true,se=TRUE)
theta.mif <- t(sapply(mf,function(x)coef(x$mif)))
loglik.mif <- t(sapply(mf,function(x)logmeanexp(x$ll,se=TRUE)))
best <- which.max(loglik.mif[,1])
theta.mif <- theta.mif[best,]
loglik.mif <- loglik.mif[best,]
rbind(
  mle=c(signif(theta.mif[estpars],3),loglik=round(loglik.mif,2)),
  truth=c(signif(theta.true[estpars],3),loglik=round(loglik.true,2))
  ) -> results.table
```

Each of the `r length(mf)` `mif` runs ends up at a different place.
In this case (but by no means in every case), we can average across these parameter estimates to get an approximate maximum likelihood estimate.
We'll evaluate the likelihood several times at this estimate to get an idea of the Monte Carlo error in our likelihood estimate.
The particle filter produces an unbiased estimate of the likelihood;
therefore, we'll average the likelihoods, not the log likelihoods.


Convergence plots can be used to help diagnose convergence of the iterated filtering algorithm.
The following shows part of the output of `plot(mf)`.

```{r mif2-plot,echo=FALSE,cache=FALSE,fig.height=6}
op <- par(mfrow=c(4,1),mar=c(3,3,0,0),mgp=c(2,1,0),bty='l')
loglik <- sapply(mf,function(x)conv.rec(x$mif,"loglik"))
log.r <- sapply(mf,function(x)conv.rec(x$mif,"r"))
log.sigma <- sapply(mf,function(x)conv.rec(x$mif,"sigma"))
log.tau <- sapply(mf,function(x)conv.rec(x$mif,"tau"))
matplot(loglik,type='l',lty=1,xlab="",ylab=expression(log~L),xaxt='n',ylim=max(loglik,na.rm=T)+c(-12,3))
matplot(log.r,type='l',lty=1,xlab="",ylab=expression(log~r),xaxt='n')
matplot(log.sigma,type='l',lty=1,xlab="",ylab=expression(log~sigma),xaxt='n')
matplot(log.tau,type='l',lty=1,xlab="MIF iteration",ylab=expression(log~tau))
par(op)
```

Here is a summary of the results.
```{r first-mif-results-table,echo=FALSE,cache=FALSE}
print(results.table)
```

## References
